import requests
import os
import re
from urllib.parse import unquote
import time

# ====================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
# ====================================================================
# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¬Ø§Ù…Ø¹ Ø¯Ø± Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨
SEARCH_KEYWORDS = [
    'vless', 'vmess', 'trojan', 'ss', 'ssr', 
    'subscribe', 'v2ray', 'proxy list', 'proxies', 'clash'
    'sub', 'hysteria', 'hysteria2', 'socks', 'wireguard'
] + list('abcdefghijklmnopqrstuvwxyz0123456789')

EXISTING_SOURCES_FILE = "merge_configs.py"
OUTPUT_FILE = "discovered_sources.txt"
CRAWLED_URLS_STATE_FILE = "crawled_urls.txt"
GITHUB_TOKEN = os.getenv("GH_PAT")

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ---
MAX_DEPTH = 10
REQUEST_TIMEOUT = 15
TOTAL_TIMEOUT_SECONDS = 5 * 60 * 60  # ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ú©Ù„ÛŒ Ûµ Ø³Ø§Ø¹Øª

# --- Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
START_TIME = time.time()
URL_REGEX = re.compile(r'https?://raw\.githubusercontent\.com/[^\s"\'`<>]+')
GENERAL_URL_REGEX = re.compile(r'https?://[^\s"\'`<>]+')
PROXY_PROTOCOL_REGEX = re.compile(r'^(vmess|vless|ss|ssr|trojan|hysteria|hysteria2|tuic|brook|socks|wireguard)://')

# ====================================================================

def clean_url(url):
    """URL Ø±Ø§ Ø¨Ø§ Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø¹Ø¯ Ø§Ø² .txt Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    try:
        txt_position = url.index('.txt')
        return url[:txt_position + 4]
    except ValueError:
        return url

def extract_urls_from_file(file_path):
    """ØªÙ…Ø§Ù… URLÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±Ø¬Ú©Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    urls = set()
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            found_urls = GENERAL_URL_REGEX.findall(content)
            urls.update(found_urls)
    except FileNotFoundError:
        print(f"Warning: {file_path} not found.")
    return urls

def is_timeout():
    """Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø²Ù…Ø§Ù† Ú©Ù„ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡."""
    if time.time() - START_TIME > TOTAL_TIMEOUT_SECONDS:
        print("â° Global timeout reached. Finalizing the process...")
        return True
    return False

def load_state(file_path):
    """Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² URLÙ‡Ø§ Ø±Ø§ Ø§Ø² ÛŒÚ© ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if not os.path.exists(file_path):
        return set()
    with open(file_path, "r", encoding="utf-8") as f:
        return {line.strip() for line in f}

def save_state(urls, file_path):
    """Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² URLÙ‡Ø§ Ø±Ø§ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ Ù…Ø´Ø®Øµ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    with open(file_path, "w", encoding="utf-8") as f:
        if urls:
            for url in sorted(list(urls)):
                f.write(url + "\n")

def search_github_paginated(query, token):
    """Ø¯Ø± API Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø¬Ø³ØªØ¬Ùˆ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ù…Ù…Ú©Ù† Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    headers = {"Accept": "application/vnd.github.v3+json"}
    if token:
        headers["Authorization"] = f"token {token}"
    
    all_items = []
    # Ú©ÙˆØ¦Ø±ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø­Ø§ÙˆÛŒ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ù‡Ø³ØªÙ†Ø¯
    search_query = f'"{query}" in:file extension:txt extension:md extension:json'
    
    for page in range(1, 11):  # Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û° ØµÙØ­Ù‡ (Û±Û°Û°Û° Ù†ØªÛŒØ¬Ù‡) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©ÙˆØ¦Ø±ÛŒ
        if is_timeout(): break
        
        params = {"q": search_query, "per_page": 100, "page": page}
        
        try:
            response = requests.get("https://api.github.com/search/code", headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            items = data.get("items", [])
            
            if not items: break
            
            all_items.extend(items)
            
            # Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù†ØªØ§ÛŒØ¬ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ú©Ù…ÛŒ ØµØ¨Ø± Ú©Ù†
            if data.get("total_count", 0) > len(all_items):
                time.sleep(2) # ÙˆÙ‚ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø±Ø®

        except requests.RequestException as e:
            print(f"    -> Error during GitHub API request for query '{query}': {e}")
            break
            
    return all_items

def process_url_recursively(url, final_sources, visited_urls, depth):
    """ÛŒÚ© URL Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if (depth > MAX_DEPTH or url in visited_urls or is_timeout() or
            not url.startswith("https://raw.githubusercontent.com/")):
        return

    indent = "  " * depth
    print(f"{indent}Processing (Depth {depth}): {url[:90]}...")
    visited_urls.add(url)

    try:
        response = requests.get(url, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        content = response.text
    except requests.RequestException:
        return

    if PROXY_PROTOCOL_REGEX.search(content):
        print(f"{indent}  -> âœ… Found direct configs. Adding to final list.")
        final_sources.add(url)
        return

    nested_urls_raw = URL_REGEX.findall(content)
    if nested_urls_raw:
        cleaned_nested_urls = {clean_url(u) for u in nested_urls_raw}
        print(f"{indent}  -> ğŸ“„ Found {len(cleaned_nested_urls)} nested links to crawl.")
        for new_url in cleaned_nested_urls:
            process_url_recursively(new_url, final_sources, visited_urls, depth + 1)

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯."""
    if not GITHUB_TOKEN:
        print("âŒ ERROR: GH_PAT environment variable is not set. Exiting.")
        return

    final_sources = set()
    visited_urls = set()
    try:
        print("1. Loading previous state...")
        existing_urls_in_project = extract_urls_from_file(EXISTING_SOURCES_FILE)
        crawled_urls = load_state(CRAWLED_URLS_STATE_FILE)
        visited_urls = existing_urls_in_project.union(crawled_urls)
        print(f"Loaded {len(visited_urls)} previously known or crawled URLs.")

        print("\n2. Searching GitHub for initial seed URLs...")
        initial_seed_urls = set()
        for keyword in SEARCH_KEYWORDS:
            if is_timeout(): break
            print(f"\n--- Searching for files containing keyword: '{keyword}' ---")
            
            search_results = search_github_paginated(keyword, GITHUB_TOKEN)
            
            for item in search_results:
                raw_url = item.get("html_url").replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                cleaned_url = clean_url(unquote(raw_url))
                initial_seed_urls.add(cleaned_url)
            
            print(f"Found {len(search_results)} items. Total unique seeds: {len(initial_seed_urls)}")
            time.sleep(3) # ÙˆÙ‚ÙÙ‡ Ø¨ÛŒØ´ØªØ± Ø¨ÛŒÙ† Ù‡Ø± Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
        
        crawl_targets = initial_seed_urls - visited_urls
        print(f"\n3. Starting deep crawl from {len(crawl_targets)} new unique seed URLs...")
        for url in list(crawl_targets):
            if is_timeout(): break
            process_url_recursively(url, final_sources, visited_urls, depth=1)

    finally:
        print("\nSaving state before exiting...")
        save_state(visited_urls, CRAWLED_URLS_STATE_FILE)
        print(f"Saved {len(visited_urls)} total crawled URLs to '{CRAWLED_URLS_STATE_FILE}'.")

        new_final_sources = final_sources - existing_urls_in_project
        
        if new_final_sources:
            all_discovered = load_state(OUTPUT_FILE).union(new_final_sources)
            save_state(all_discovered, OUTPUT_FILE)
            print(f"\nDiscovered {len(new_final_sources)} new final source URLs in this run!")
            print(f"âœ… Successfully updated '{OUTPUT_FILE}'.")
        else:
            print("\nâœ… No new sources found in this run.")

if __name__ == "__main__":
    main()
