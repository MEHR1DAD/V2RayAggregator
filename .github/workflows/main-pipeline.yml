# نام ربات اصلی ما
name: Main Proxy Pipeline

on:
  # هر ۳ ساعت یک‌بار به صورت خودکار اجرا می‌شود
  schedule:
    - cron: '0 */3 * * *'
  # امکان اجرای دستی را هم فراهم می‌کند
  workflow_dispatch:

# تضمین می‌کند که فقط یک نمونه از این ربات در هر لحظه در حال اجرا باشد
concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  #----------------------------------------------------
  # مرحله ۱: آماده‌سازی و بسته‌بندی کارها
  #----------------------------------------------------
  prepare-work:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Dependencies
        run: pip install pyyaml httpx requests

      - name: Run Data Acquisition and Batching
        env:
          GH_PAT: "${{ secrets.GH_PAT }}"
        run: |
          python manage_sources.py
          python merge_configs.py
          python prepare_batches.py
      
      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: work-batches
          path: ci_batches/

  #-----------------------------------------------------------------
  # مرحله ۲: اجرای تست‌های موازی (لایه اول: زنده بودن)
  #-----------------------------------------------------------------
  run-liveness-tests:
    needs: prepare-work
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        batch_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # <<< FIX: کتابخانه‌های requests و geoip2 اضافه شدند
      - name: Install Dependencies
        run: pip install pyyaml requests geoip2-database

      - name: Download Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          name: work-batches
          path: ci_batches/
      
      - name: Run Liveness Check on Assigned Batch
        run: |
          BATCH_FILE="ci_batches/batch_${{ matrix.batch_id }}.txt"
          CANDIDATES_FILE="live_candidates_${{ matrix.batch_id }}.txt"
          python check_liveness.py --input $BATCH_FILE --output $CANDIDATES_FILE
      
      - name: Upload Live Candidates Artifact
        uses: actions/upload-artifact@v4
        with:
          name: live-candidates-${{ matrix.batch_id }}
          path: live_candidates_${{ matrix.batch_id }}.txt

  #------------------------------------------------------------------
  # مرحله ۳: اجرای تست‌های موازی (لایه دوم: تست سرعت)
  #------------------------------------------------------------------
  run-speed-tests:
    needs: run-liveness-tests
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        batch_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install requests geoip2-database pyyaml httpx

      - name: Download Cached Dependencies
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh release download deps/geoip-latest --repo ${{ github.repository }} --pattern '*.mmdb' --clobber
          LATEST_XRAY_PKG_TAG=$(gh release list --repo ${{ github.repository }} --limit 50 --json tagName --jq '.[] | select(.tagName | startswith("deps/xray-pkg-")) | .tagName' | head -n 1)
          if [ -z "$LATEST_XRAY_PKG_TAG" ]; then echo "::error::No cached Xray package found."; exit 1; fi
          gh release download "$LATEST_XRAY_PKG_TAG" --repo ${{ github.repository }} --clobber
          chmod +x xray
      
      - name: Download Live Candidates Artifact
        uses: actions/download-artifact@v4
        with:
          name: live-candidates-${{ matrix.batch_id }}
      
      - name: Run Speed Test on Live Candidates
        run: |
          CANDIDATES_FILE="live_candidates_${{ matrix.batch_id }}.txt"
          DB_FILE="results_${{ matrix.batch_id }}.db"
          python test_worker.py --input $CANDIDATES_FILE --db-file $DB_FILE

      - name: Upload Result Artifact
        uses: actions/upload-artifact@v4
        with:
          name: db-result-${{ matrix.batch_id }}
          path: results_${{ matrix.batch_id }}.db
  
  #-------------------------------------------------------------
  # مرحله ۴: ادغام نتایج، ساخت خروجی‌ها و انتشار
  #-------------------------------------------------------------
  merge-and-publish:
    runs-on: ubuntu-latest
    needs: run-speed-tests
    # این job همیشه اجرا می‌شود، حتی اگر بعضی از تست‌ها فیل شده باشند
    if: success() || failure()
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install pyyaml jdatetime pytz

      - name: Download All Result Artifacts
        uses: actions/download-artifact@v4
        with:
          path: db_results/

      - name: Merge, Generate, and Finalize
        run: |
          python merge_databases.py --input-dir db_results/ --output-db aggregator_data.db
          python create_rotating_sub.py
          python generate_readme.py
      
      - name: Commit and Push Final Changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action (Main Pipeline)"
          git add .
          if ! git diff --staged --quiet; then
            git commit -m "feat(auto): update proxy configurations and README"
            git pull --rebase
            git push
          else
            echo "No changes to commit."
          fi
